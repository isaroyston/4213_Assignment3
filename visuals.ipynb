{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b8f6fb2",
   "metadata": {},
   "source": [
    "# Results Visualization\n",
    "Comparing Full Fine-tuning vs LoRA on Financial Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e555a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(\"full_finetuning_results/results_summary.json\") as f:\n",
    "    full_results = json.load(f)\n",
    "with open(\"lora_results/results_summary.json\") as f:\n",
    "    lora_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96031408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "metrics = ['eval_accuracy', 'eval_f1_macro', 'eval_precision', 'eval_recall']\n",
    "metric_names = ['Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    full_val = full_results['validation'][metric]\n",
    "    full_test = full_results['test'][metric]\n",
    "    lora_val = lora_results['validation'][metric]\n",
    "    lora_test = lora_results['test'][metric]\n",
    "    \n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[idx].bar(x - width/2, [full_val, full_test], width, label='Full Fine-tuning', alpha=0.8)\n",
    "    axes[idx].bar(x + width/2, [lora_val, lora_test], width, label='LoRA', alpha=0.8)\n",
    "    \n",
    "    axes[idx].set_ylabel(name)\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels(['Validation', 'Test'])\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_ylim([0.78, 0.88])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (v1, v2) in enumerate(zip([full_val, full_test], [lora_val, lora_test])):\n",
    "        axes[idx].text(i - width/2, v1 + 0.003, f'{v1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        axes[idx].text(i + width/2, v2 + 0.003, f'{v2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter efficiency\n",
    "trainable = lora_results['trainable_params']\n",
    "total = lora_results['total_params']\n",
    "frozen = total - trainable\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "ax1.pie([trainable, frozen], labels=[f'Trainable\\n{trainable:,}', f'Frozen\\n{frozen:,}'], \n",
    "        colors=colors, autopct='%1.2f%%', startangle=90, explode=(0.1, 0))\n",
    "ax1.set_title('LoRA Parameter Distribution', fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "methods = ['Full Fine-tuning', 'LoRA']\n",
    "params = [total, trainable]\n",
    "bars = ax2.bar(methods, params, color=colors, alpha=0.8)\n",
    "ax2.set_ylabel('Trainable Parameters')\n",
    "ax2.set_title('Parameter Efficiency Comparison', fontweight='bold')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, param in zip(bars, params):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{param:,}\\n({param/total*100:.2f}%)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('efficiency_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Efficiency: {(1 - trainable/total)*100:.2f}% reduction in trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "ax.axis('off')\n",
    "\n",
    "metrics = ['Accuracy', 'F1 (Macro)', 'F1 (Weighted)', 'Precision', 'Recall', 'Loss']\n",
    "metric_keys = ['eval_accuracy', 'eval_f1_macro', 'eval_f1_weighted', 'eval_precision', 'eval_recall', 'eval_loss']\n",
    "\n",
    "table_data = []\n",
    "for name, key in zip(metrics, metric_keys):\n",
    "    table_data.append([\n",
    "        name,\n",
    "        f\"{full_results['validation'][key]:.4f}\",\n",
    "        f\"{full_results['test'][key]:.4f}\",\n",
    "        f\"{lora_results['validation'][key]:.4f}\",\n",
    "        f\"{lora_results['test'][key]:.4f}\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                colLabels=['Metric', 'Full (Val)', 'Full (Test)', 'LoRA (Val)', 'LoRA (Test)'],\n",
    "                cellLoc='center', loc='center', colWidths=[0.2, 0.15, 0.15, 0.15, 0.15])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    for j in range(5):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#f0f0f0')\n",
    "\n",
    "plt.savefig('results_table.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics heatmap\n",
    "metrics = ['Accuracy', 'F1 Macro', 'Precision', 'Recall']\n",
    "metric_keys = ['eval_accuracy', 'eval_f1_macro', 'eval_precision', 'eval_recall']\n",
    "\n",
    "data = [[full_results['validation'][k], full_results['test'][k], \n",
    "         lora_results['validation'][k], lora_results['test'][k]] for k in metric_keys]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "im = ax.imshow(data, cmap='YlGnBu', aspect='auto', vmin=0.80, vmax=0.88)\n",
    "\n",
    "ax.set_xticks(np.arange(4))\n",
    "ax.set_yticks(np.arange(len(metrics)))\n",
    "ax.set_xticklabels(['Full (Val)', 'Full (Test)', 'LoRA (Val)', 'LoRA (Test)'])\n",
    "ax.set_yticklabels(metrics)\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, f'{data[i][j]:.3f}', ha=\"center\", va=\"center\", \n",
    "                color=\"black\", fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_title('Performance Metrics Heatmap', fontweight='bold', pad=15)\n",
    "plt.colorbar(im, ax=ax, label='Score')\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba85dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Full Fine-tuning Test Accuracy: {full_results['test']['eval_accuracy']:.4f}\")\n",
    "print(f\"LoRA Test Accuracy: {lora_results['test']['eval_accuracy']:.4f}\")\n",
    "print(f\"\\nParameter Efficiency: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)\")\n",
    "print(\"\\nâœ… Visualizations saved: metrics_comparison.png, efficiency_comparison.png, results_table.png, metrics_heatmap.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
